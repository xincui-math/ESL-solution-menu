
\section*{Chapter 3}
\subsection*{Ex. 3.1}
For simplicity, let's denote variable with tilde as skipping skipping column $i$.
Here we explore bit more in this problem to clarify how to compute F-score.
\begin{itemize}
    \item $RSS_0$: $(y - X\beta)^T(y - X\beta)$
    \item $RSS_1$: $(y - \tilde{X}\tilde{\beta})^T(y - \tilde{X}\tilde{\beta})$
    \item $rss_1$: $(y - X\beta + X_i\beta_i)^T(y - X\beta + X_i\beta_i)$
\end{itemize}

or say, $RSS_1$ uses refit $\tilde{\beta}$, $rss_1$ uses original $\beta$.
$$rss_1 - RSS_0 = X_i^TX_i\beta_i^2 = (X^TX)_{ii}\beta_{i}^2.$$

Denote
\begin{itemize}
    \item $span{\tilde{X}} = span\{X_{k}|k\neq i\}$
    \item $span{X} = span\{X_{k}\}$
    \item $P_{A}$, projection matrix to $span{A}.$
    \item $\tilde{X}^{\perp}$ is the orthogonal complement of $span\{\tilde{X}\}$ inside $span\{X\}$.
\end{itemize}

$$RSS_1 - RSS_0=y^T P_{\tilde{X}}y - y^T P_{X}y=y^T P_{\tilde{X}^{\perp}}y.$$

$$F_{i} = \frac{(RSS_1 - RSS_0)/(p_1 - p_0)}{RSS_1 / (N - p_1 -1)}=\frac{y^T P_{\tilde{X}^{\perp}}y}{\hat{\sigma}^2}=\frac{x_{i}^T P_{\tilde{X}^{\perp}}x_{i}}{\hat{\sigma}^2}\hat{\beta{i}}^2=\frac{\Vert X_{i}^{\perp}\Vert^2}{\hat{\sigma}^2}\hat{\beta{i}}^2.$$

$$z_{i}^2 = \frac{\hat{\beta_i}^2}{\hat{\sigma}^2(X^TX)^{-1}_{ii}}.$$

Now let's compute $(X^TX)^{-1}_{ij}$. For arbitrary $\epsilon\sim\mathcal{N}(0, I_{N})$,
\begin{eqnarray*}
    & &(X^TX)^{-1}_{ij}\\
    &=& COV\left((X^TX)^{-1}X^T \epsilon, (X^TX)^{-1}X^T \epsilon\right)\\
    &=& \mathbb{E}(\beta_{\epsilon, X, {i}} \beta_{\epsilon, X, {j}})\\
    &=& \frac{X_{i}^{\perp}\cdot X_{j}^{\perp}}{\Vert X_{i}^{\perp}\Vert^2 \Vert X_{j}^{\perp}\Vert^2}
\end{eqnarray*}
Thus we have $z_{i}^2 = \frac{\hat{\beta_i}^2}{\hat{\sigma}^2(X^TX)^{-1}_{ii}}= \frac{\hat{\beta_i}^2\Vert X_{i}^{\perp}\Vert^2}{\hat{\sigma}^2}=F_{i}.$ And $f_{i}=\frac{\hat{\beta_i}^2(X^tX)_{ii}}{\hat{\sigma}^2}$


\subsection*{Ex. 3.2}

\begin{itemize}
    \item $\mbox{Var}(a^{T}\beta) = a^{T}COV_\beta a = (X^{T}X)^{-1}\sigma^2$.
    \item $C_{\beta}=\{(\hat{\beta} - \beta)^T(X^TX)(\hat{\beta} - \beta)\leq \hat{\sigma}^2 {\chi_{p+1}^2}^{(1-\alpha)}\}$
\end{itemize}

The first estimation needs $\sigma$, the second one doesn't. (code TBD)

\subsection*{Ex. 3.3}
For quantity $a^{T}\beta$, we have unbias estimator $\hat{\theta} = a^T\hat{\beta} = a^T (X^{T}X)^{-1}X^{T}y.$ Giving another unbiased estimation $c^{T}y$, write $c^{T} = a^{T}(X^{T}X)^{-1}X^{T} + D$.
$$\mathbb{E}(\left[a^{T}(X^{T}X)^{-1}X^{T} + D\right]y)=a^{T}\beta + DX\beta.$$
Hence we have $DX = 0$.
\begin{eqnarray*}
    & & \var(c^{T}y) \\
    &=& \left[a^{T}(X^{T}X)^{-1}X^{T} + D\right]\left[a^{T}(X^{T}X)^{-1}X^{T} + D\right]^T \sigma^2\\
    &=& [a^{T}(X^{T}X)^{-1}a + DD^T]\sigma^2 \\
    &\succeq& a^{T}(X^{T}X)^{-1}a\sigma^2 \\
    &=& \var(a^T\hat{\beta}).
\end{eqnarray*}

\subsection*{Ex. 3.4}
Give $X = (X_0, X_1, ..., X_{n - 1})$ and $y$.
\begin{itemize}
    \item $\tilde{\beta_0} = \frac{\cov(X_0, y)}{\cov(X_0, X_0)}$
    \item $\tilde{\beta_i} = \frac{\cov(z_i, y)}{\cov(z_i, z_i)}$ with $z_i = x_i - \sum_{j=0}^{i-1} \gamma_{ij} x_j$, $\gamma_{ij} = \frac{\cov(x_i, x_j)}{\cov(x_j, x_j)}.$
\end{itemize}
$$y = \sum_{i=0}^{n}\tilde{\beta_i}z_{i} = \sum_{i=1}^{n}\tilde{\beta_i}z_{i}= \sum_{i=1}^{n}\tilde{\beta_i}(x_i - \sum_{j=1}^{i-1} \gamma_{ij} x_j)= \sum_{i=1}^{n}(\tilde{\beta_i}-\sum_{j=i+1}^{n}\Gamma_{ji})x_i.$$

\subsection*{Ex. 3.5}
\begin{itemize}
    \item $\beta_0^c = \beta_0 + \sum_{j=0}^p\bar{x_j}\beta_j$
    \item $\beta_i^c = \beta_i$
\end{itemize}

\subsection*{Ex. 3.6}
Assume prior $\beta \sim N(0, \tau I)$, data samples from $y\sim N(X\beta, \sigma^2 I)$. Posterior distribution has PDF proportional to:
$$p(\beta|D) \sim  exp\left[-\frac{(y-X\beta)^T(y-X\beta)}{2\sigma^2}-\frac{\beta^T\beta}{2\tau^2}\right]$$

Q1: Equivalent to mode: $\lambda = \frac{\sigma^2}{\tau^2}.$

Q2: Equivalent to posterior mean:
$$-\frac{1}{2}(\beta - m_1)^Tm_2^{-1}(\beta - m_1) = -\frac{(y-X\beta)^T(y-X\beta)}{2\sigma^2}-\frac{\beta^T\beta}{2\tau^2}$$
Solves the system, we have,
\[
    \begin{cases}
        m_1=(\frac{X^TX}{\sigma^2} + \frac{I}{\tau^2}) \\
        m_2=(X^TX + \frac{\sigma^2}{\tau^2}X^Ty).
    \end{cases}
\]

\subsection*{Ex. 3.7}
Direct consequence of $p(\beta|D) \sim  exp\left[-\frac{(y-X\beta)^T(y-X\beta)}{2\sigma^2}-\frac{\beta^T\beta}{2\tau^2}\right]$.

\subsection*{Ex. 3.8}
For special matrix, $X=(e, x_1, x_2, x_3,... , x_p)$, centered matrix is given by
$$\widetilde{X} = (x_1 - \overline{x_1}, ..., x_p - \overline{x_p}).$$
Gram-Schmidt processes gives the following.
$x_i = \sum_{d=1}^{i - 1}q_{d}r_{di} + \frac{\langle e, x_i \rangle}{|e|}\frac{e}{|e|}$.
$$Q_{lower}R_{lower} = \widetilde{X} = U\Sigma V^{*}.$$
Hence column span of $Q_{lower}$ is the same as column span of U.

Denote $\tilde{R} = \Sigma^{-1} R =  V^{*}.$ Consider $r_i$ be $i$-th column vector of $\tilde{R}$. Then use induction not hard to see $\tilde{R}$ is diagonal. This implies $\Sigma V$ is diagonal of 1/-1.

\subsection*{Ex. 3.9}
Denote $z_{i} = x_{i} - \sum_{k=1}^r q_{k}( x_{i}, q_{k})$, variance explained increment has norm,
$$\Vert\hat{\beta_{i}}z_{i}\Vert = |\langle y, x_{i} - \sum_{k=1}^r q_{k} (x_{i}, q_{k}) \rangle|.$$
For new set of feature it is equivalent (and faster) to pick the following,
$$\mbox{argmax}_{i}\Vert (y^T X - y^TQQ^{T}X)_{i}\Vert.$$
\subsection*{Ex. 3.10}
Exercise 3.1 shows $F$ statistics for dropping i-th variable corresponds to $z^2_i$. Hence we just need to drop the variable with lowest $|z|$.

\subsection*{Ex. 3.11}
\begin{eqnarray*}
    && \left(\frac{d tr[(Y-XB)^{T}(Y-XB)]}{dB}\right)_{ij}\\
    &=& \frac{d}{db_{ij}}\sum_{p,q,v}(y_{pq}-X_{pv}B_{vq})^2\\
    &=& -\sum_{p,q,v}2(y_{pq}-X_{pv}B_{vq})X_{ps}\delta_{sq}^{ij}\\
    &=& -\sum_{p,v}2(y_{pj}-X_{pv}B_{vj})X_{ps}\delta_{s}^{i}\\
    &=& -\sum_{p,v}2(y_{pj}-X_{pv}B_{vj})X_{pi}\\
    &=& -2(X^TY -X^{T}XB)_{ij}.
\end{eqnarray*}
Consider symmetric square root $\Sigma^{-\frac{1}{2}}$, the solution is
$$X^TY\Sigma^{-\frac{1}{2}} -X^{T}XB\Sigma^{-\frac{1}{2}}=0.$$

\subsection*{Ex. 3.12}

$$
    \left(
    \begin{array}{c}
            y \\
            0 \\
        \end{array}
    \right)
    =
    \left(
    \begin{array}{c}
            \widetilde{X}    \\
            \sqrt{\lambda} I \\
        \end{array}
    \right)
    \beta + \epsilon.
$$
$RSS(\beta)=(y-\tilde{X}\beta)^T(y-\tilde{X}\beta)  + \lambda \Vert\beta\Vert^2$, same as Ridge regression.

\subsection*{Ex. 3.13}
Now consider $z_{i}=Xv_{i}=\lambda_{i}u_i$.
\begin{itemize}
    \item $\langle z_{i}, y\rangle=\lambda_{i}\langle u_{i}, y\rangle$.
    \item $\langle z_{i}, z_{i}\rangle=\lambda_{i}^2$.
\end{itemize}
$$\hat{\beta}^{pcr}(p)=\sum_{i=1}^p\frac{\langle z_{i}, y\rangle}{\langle z_{i}, z_{i}\rangle}v_i=VD^{-1}U^{T}y=\hat{\beta}^{ls}.$$

\subsection*{Ex. 3.14}
\begin{itemize}
    \item $z_{1} = \sum_{j=1}^{p}\langle x_{j}, y\rangle x_{j}$.
    \item $x_{j}^{1}=x_{j}^0-\widehat{\phi_{1j}}\frac{z_1}{\langle z_1, z_1\rangle}$
    \item $\langle x_{j}^{1}, y \rangle=\langle x_{j}^0-\widehat{\phi_{1j}}\frac{z_1}{\langle z_1, z_1\rangle}, y\rangle=\widehat{\phi_{1j}} - \widehat{\phi_{1j}}\frac{\langle z_1, y\rangle}{\langle z_1, z_1\rangle}. $
\end{itemize}
Using $x_{i}$ are orthogonal, we have,
$$\langle z_1, y\rangle = \langle z_1, z_1\rangle=\sum_{j=1}^p \widehat{\phi_{1j}}^2.$$
This implies $\widehat{\phi_{2j}} = \langle x_{j}^{1}, y \rangle =0.$

\subsection*{Ex. 3.15}
(PLS)
$$\max_{\alpha}\mbox{corr}^2(y, X\alpha)\var(X\alpha)$$
$$\mbox{subject to }\Vert\alpha\Vert=1, \alpha^T S \phi_{l}=0.$$

The problem is equivalent to the following.
$$\max_{\alpha}\cov(y, X\alpha)$$
$$\mbox{subject to }\Vert\alpha\Vert=1, \alpha^T S \phi_{l}=0.$$

Decompose $\mbox{span}(X)=X_{proj}\oplus X_{ortho}$ using inner product. Restrict $\alpha$ to $(X\alpha)_{proj}=0$, Lagrange multiplier gives,
\begin{eqnarray*}
    && L(x, \lambda) \\
    &=& \cov(y, X\alpha) - \lambda \alpha^T \alpha\\
    &=& \sum_{i}\left[\langle y, x_{iortho}\rangle \alpha_{i}- \lambda \alpha_i^2\right] + \langle y, (X\alpha)_{proj}\rangle.
\end{eqnarray*}
This implies $\alpha_i\sim \langle y, x_{iortho}\rangle.$

\subsection*{Ex. 3.16}
Consider $y = \sum_{i} \alpha_i x_i + \epsilon$, where $\langle x_{i}, x_{j}\rangle=\delta_{ij}.$

Notice for any $S$, we always have estimated $\widehat{\beta_{i}^{(S)}} =\widehat{\beta_i}=\langle y, x_i\rangle$.

(1) Best $M$-subset
\begin{eqnarray*}
    SSR(S)&=&\left\Vert \sum_{i\in S}\widehat{\beta_{i}}x_{i}\right\Vert^2\\
    &=&\sum_{i\in S}\widehat{\beta_{i}}^2.
\end{eqnarray*}
It is equivalent to pick the largest $M$ $\widehat{\beta_{i}}$ in full regression.

(2) Ridge.
\begin{eqnarray*}
    &&\min\{\Vert y-X\beta\Vert^2 + \lambda \Vert \beta\Vert^2\}\\
    &=&\min_{\beta}\sum_{i}[(\lambda + 1)\beta_{i}^2 - 2\langle x_{i}, y\rangle\beta_{i}]
\end{eqnarray*}

Hence $\beta_{i}^{Ridge}=\frac{\widehat{\beta_i}}{\lambda + 1}.$


(3) Lasso.
\begin{eqnarray*}
    &&\min\{\Vert y-X\beta\Vert^2 + 2 \lambda \Vert \beta\Vert\}\\
    &=&\min_{\beta}\sum_{i}\left[\beta_{i}^2 - 2[\hat{\beta_{i}} - \lambda \mbox{sign}(\beta_i)]\beta_{i}\right]
\end{eqnarray*}

(I) $\beta_{i} > 0$
\begin{itemize}
    \item $\hat{\beta_{i}} \geq \lambda$: $\beta_{i} = \hat{\beta_{i}} - \lambda$
    \item $\hat{\beta_{i}} < \lambda$: $\beta_{i} = 0$
\end{itemize}

(II) $\beta_{i} < 0$
\begin{itemize}
    \item $\hat{\beta_{i}} \geq -\lambda$: $\beta_{i} = 0$
    \item $\hat{\beta_{i}} < \lambda$: $\beta_{i} = \hat{\beta_{i}} + \lambda$
\end{itemize}
Rephrase the above analysis gives $\beta^{Lasso}_{j} = \mbox{sign}(\hat{\beta}_{j})(\hat{\beta}_{j} - \lambda)_+$.

\subsection*{Ex. 3.17}
Code TBD.

\subsection*{Ex. 3.18}
Solving $\beta$ is equivalent represent $y_{proj}$ in coordinate $X$. The PLS solves a set of orthonormal basis iteratively for space $\mbox{span}\{X\},$ namely $z_{m}.$ Under $z_{m}$,
$$\beta_{z_m}=\frac{\langle z_{m}, y\rangle}{\langle z_{m}, z_{m}\rangle}_{I}.$$
This formula matches the conjugate gradients algorithm.

\subsection*{Ex. 3.19}
(1) $L^2$ norm as a decreasing function of $\lambda$ in Ridge regression.
\begin{eqnarray*}
    &&\frac{d}{d\lambda} \Vert \beta^{ridge}\Vert^2\\
    &=&\frac{d}{d\lambda} [y^TX(X^TX + \lambda I)^{-1}(X^TX + \lambda I)^{-1}X^Ty]\\
    &=&-2\lambda [y^TX(X^TX + \lambda I)^{-1}(X^TX + \lambda I)^{-1}(X^TX + \lambda I)^{-1}X^Ty]\\
    &=&-2\lambda \beta_{\lambda}^T(X^TX + \lambda I)^{-1}\beta_{\lambda} \\
    &\leq& 0
\end{eqnarray*}
(2) $L^1$ norm may not be a decreasing function of $\lambda$ in Lasso regression.

\subsection*{Ex. 3.20}
(CCR problem) Follow the exact same approach in \cite{Wiki_CCR}.
$c = \Sigma_{YY}^{1/2}u$, $d = \Sigma_{XX}^{1/2}v$.
Cauchy Schwarz gives,
$$u^TY^TXv\leq\frac{\Vert \Sigma_{XX}^{-1/2}\Sigma_{XY}\Sigma_{YY}^{-1/2}c \Vert}{\Vert c \Vert}.$$
Equality holds when $d = \lambda \Sigma_{XX}^{-1/2}\Sigma_{XY}\Sigma_{YY}^{-1/2}c.$ Perform SVD decomposition on $ \Sigma_{XX}^{-1/2}\Sigma_{XY}\Sigma_{YY}^{-1/2} = UD V^{T}$, and consider $\tilde{c} = V^T c.$
$$\frac{\Vert \Sigma_{XX}^{-1/2}\Sigma_{XY}\Sigma_{YY}^{-1/2}c \Vert}{\Vert c \Vert}=\frac{\Vert\tilde{c}^T \Sigma \tilde{c}\Vert}{\Vert \tilde{c}\Vert}.$$
Optimization result $\tilde{c}$ gives identity matrix. Hence $c$ gives $V$, left singular vectors, equality condition gives $d$ are right singular vectors of $\Sigma_{XX}^{-1/2}\Sigma_{XY}\Sigma_{YY}^{-1/2}$.

