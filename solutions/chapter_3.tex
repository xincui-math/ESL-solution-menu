
\section*{Chapter 3}
\subsection*{Ex. 3.1}
For simplicity, let's denote variable with tilde as skipping skipping column $i$.
Here we explore bit more in this problem to clarify how to compute F-score.
\begin{itemize}
	\item $RSS_0$: $(y - X\beta)^T(y - X\beta)$
	\item $RSS_1$: $(y - \tilde{X}\tilde{\beta})^T(y - \tilde{X}\tilde{\beta})$
	\item $rss_1$: $(y - X\beta + X_i\beta_i)^T(y - X\beta + X_i\beta_i)$
\end{itemize}

or say, $RSS_1$ uses refit $\tilde{\beta}$, $rss_1$ uses original $\beta$.
$$rss_1 - RSS_0 = X_i^TX_i\beta_i^2 = (X^TX)_{ii}\beta_{i}^2.$$

Denote
\begin{itemize}
	\item $span{\tilde{X}} = span\{X_{k}|k\neq i\}$
	\item $span{X} = span\{X_{k}\}$
	\item $P_{A}$, projection matrix to $span{A}.$
	\item $\tilde{X}^{\perp}$ is the orthogonal complement of $span\{\tilde{X}\}$ inside $span\{X\}$.
\end{itemize}

$$RSS_1 - RSS_0=y^T P_{\tilde{X}}y - y^T P_{X}y=y^T P_{\tilde{X}^{\perp}}y.$$

$$F_{i} = \frac{(RSS_1 - RSS_0)/(p_1 - p_0)}{RSS_1 / (N - p_1 -1)}=\frac{y^T P_{\tilde{X}^{\perp}}y}{\hat{\sigma}^2}=\frac{x_{i}^T P_{\tilde{X}^{\perp}}x_{i}}{\hat{\sigma}^2}\hat{\beta{i}}^2=\frac{\Vert X_{i}^{\perp}\Vert^2}{\hat{\sigma}^2}\hat{\beta{i}}^2.$$

$$z_{i}^2 = \frac{\hat{\beta_i}^2}{\hat{\sigma}^2(X^TX)^{-1}_{ii}}.$$

Now let's compute $(X^TX)^{-1}_{ij}$. For arbitrary $\epsilon\sim\mathcal{N}(0, I_{N})$,
\begin{eqnarray*}
	& &(X^TX)^{-1}_{ij}\\
	&=& COV\left((X^TX)^{-1}X^T \epsilon, (X^TX)^{-1}X^T \epsilon\right)\\
	&=& \mathbb{E}(\beta_{\epsilon, X, {i}} \beta_{\epsilon, X, {j}})\\
	&=& \frac{X_{i}^{\perp}\cdot X_{j}^{\perp}}{\Vert X_{i}^{\perp}\Vert^2 \Vert X_{j}^{\perp}\Vert^2}
\end{eqnarray*}
Thus we have $z_{i}^2 = \frac{\hat{\beta_i}^2}{\hat{\sigma}^2(X^TX)^{-1}_{ii}}= \frac{\hat{\beta_i}^2\Vert X_{i}^{\perp}\Vert^2}{\hat{\sigma}^2}=F_{i}.$ And $f_{i}=\frac{\hat{\beta_i}^2(X^tX)_{ii}}{\hat{\sigma}^2}$


\subsection*{Ex. 3.2}

\begin{itemize}
	\item $\mbox{Var}(a^{T}\beta) = a^{T}COV_\beta a = (X^{T}X)^{-1}\sigma^2$.
	\item $C_{\beta}=\{(\hat{\beta} - \beta)^T(X^TX)(\hat{\beta} - \beta)\leq \hat{\sigma}^2 {\chi_{p+1}^2}^{(1-\alpha)}\}$
\end{itemize}

The first estimation needs $\sigma$, the second one doesn't. (code TBD)

\subsection*{Ex. 3.3}
For quantity $a^{T}\beta$, we have unbias estimator $\hat{\theta} = a^T\hat{\beta} = a^T (X^{T}X)^{-1}X^{T}y.$ Giving another unbiased estimation $c^{T}y$, write $c^{T} = a^{T}(X^{T}X)^{-1}X^{T} + D$.
$$\mathbb{E}(\left[a^{T}(X^{T}X)^{-1}X^{T} + D\right]y)=a^{T}\beta + DX\beta.$$
Hence we have $DX = 0$.
\begin{eqnarray*}
& & \var(c^{T}y) \\
&=& \left[a^{T}(X^{T}X)^{-1}X^{T} + D\right]\left[a^{T}(X^{T}X)^{-1}X^{T} + D\right]^T \sigma^2\\
&=& [a^{T}(X^{T}X)^{-1}a + DD^T]\sigma^2 \\
&\succeq& a^{T}(X^{T}X)^{-1}a\sigma^2 \\
&=& \var(a^T\hat{\beta}).
\end{eqnarray*}

\subsection*{Ex. 3.4}
Give $X = (X_0, X_1, ..., X_{n - 1})$ and $y$.
\begin{itemize}
	\item $\tilde{\beta_0} = \frac{\cov(X_0, y)}{\cov(X_0, X_0)}$
    \item $\tilde{\beta_i} = \frac{\cov(z_i, y)}{\cov(z_i, z_i)}$ with $z_i = x_i - \sum_{j=0}^{i-1} \gamma_{ij} x_j$, $\gamma_{ij} = \frac{\cov(x_i, x_j)}{\cov(x_j, x_j)}.$
\end{itemize}
$$y = \sum_{i=0}^{n}\tilde{\beta_i}z_{i} = \sum_{i=1}^{n}\tilde{\beta_i}z_{i}= \sum_{i=1}^{n}\tilde{\beta_i}(x_i - \sum_{j=1}^{i-1} \gamma_{ij} x_j)= \sum_{i=1}^{n}(\tilde{\beta_i}-\sum_{j=i+1}^{n}\Gamma_{ji})x_i.$$

\subsection*{Ex. 3.5}
\begin{itemize}
    \item $\beta_0^c = \beta_0 + \sum_{j=0}^p\bar{x_j}\beta_j$
    \item $\beta_i^c = \beta_i$
\end{itemize}

\subsection*{Ex. 3.6}
Assume prior $\beta \sim N(0, \tau I)$, data samples from $y\sim N(X\beta, \sigma^2 I)$. Posterior distribution has PDF proportional to:
$$p(\beta|D) \sim  exp\left[-\frac{(y-X\beta)^T(y-X\beta)}{2\sigma^2}-\frac{\beta^T\beta}{2\tau^2}\right]$$

Q1: Equivalent to mode: $\lambda = \frac{\sigma^2}{\tau^2}.$

Q2: Equivalent to posterior mean:
$$-\frac{1}{2}(\beta - m_1)^Tm_2^{-1}(\beta - m_1) = -\frac{(y-X\beta)^T(y-X\beta)}{2\sigma^2}-\frac{\beta^T\beta}{2\tau^2}$$
Solves the system, we have,
\[
    \begin{cases}
       m_1=(\frac{X^TX}{\sigma^2} + \frac{I}{\tau^2})\\
       m_2=(X^TX + \frac{\sigma^2}{\tau^2}X^Ty).
    \end{cases}
\]

\subsection*{Ex. 3.7}
Direct consequence of $p(\beta|D) \sim  exp\left[-\frac{(y-X\beta)^T(y-X\beta)}{2\sigma^2}-\frac{\beta^T\beta}{2\tau^2}\right]$.

\subsection*{Ex. 3.8} 
For special matrix, $X=(e, x_1, x_2, x_3,... , x_p)$, centered matrix is given by 
$$\widetilde{X} = (x_1 - \overline{x_1}, ..., x_p - \overline{x_p}).$$
Gram-Schmidt processes gives the following.
$x_i = \sum_{d=1}^{i - 1}q_{d}r_{di} + \frac{\langle e, x_i \rangle}{|e|}\frac{e}{|e|}$.
$$Q_{lower}R_{lower} = \widetilde{X} = U\Sigma V^{*}.$$
Hence column span of $Q_{lower}$ is the same as column span of U. 

Denote $\tilde{R} = \Sigma^{-1} R =  V^{*}.$ Consider $r_i$ be $i$-th column vector of $\tilde{R}$. Then use induction not hard to see $\tilde{R}$ is diagonal. This implies $\Sigma V$ is diagonal of 1/-1.

\subsection*{Ex. 3.9}
Denote $z_{i} = x_{i} - \sum_{k=1}^r q_{k}( x_{i}, q_{k})$ then explained increment has norm,
$$|\hat{\beta_{i}}z_{i}| = \Vert\langle y, x_{i} - \sum_{k=1}^r q_{k} (x_{i}, q_{k}) \rangle\Vert.$$
For new set of feature it is equivalent to pick the following,
$$\mbox{argmax}_{i}\Vert (y^T X - y^TQQ^{T}X)_{i}\Vert.$$
